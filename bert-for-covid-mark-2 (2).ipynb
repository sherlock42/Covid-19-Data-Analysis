{"cells":[{"metadata":{},"cell_type":"markdown","source":"Keras Bert\n\nSimple wrapper of tf-hub Bert models for use in Keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -q https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget -q https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n!wget -q https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n!wget -q https://raw.githubusercontent.com/google-research/bert/master/tokenization.py ","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport sys\nimport zipfile\nimport modeling\nimport optimization\nimport run_classifier\nimport tokenization\n\nfrom tokenization import FullTokenizer\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_hub as hub\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model","execution_count":2,"outputs":[{"output_type":"stream","text":"WARNING: Logging before flag parsing goes to stderr.\nW0418 04:23:57.447848 140140242679168 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess = tf.Session()\n\n# Params for bert model and tokenization\nbert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\nmax_seq_length = 128","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', index_col='id')\nval_df = pd.read_csv('../input/valid.csv', index_col='id')\ntest_df = pd.read_csv('../input/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/abstract-clusters/abstract_clusters.csv',encoding='utf-8',error_bad_lines=False,engine='python')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\ndf = shuffle(df)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder().fit(df['Label'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =df['Abstract'].values","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = label_encoder.fit_transform(df['Label'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val_test, y_train, y_val_test = train_test_split(\n        X,y, test_size=0.2, random_state=0, stratify = y\n        )","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val, X_test, y_val, y_test = train_test_split(\n        X_val_test,y_val_test, test_size=0.5, random_state=0, stratify = y_val_test\n        )","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = X_train\ntrain_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = y_train\n\nval_text = X_val\nval_text = [' '.join(t.split()[0:max_seq_length]) for t in val_text]\nval_text = np.array(val_text, dtype=object)[:, np.newaxis]\nval_label = y_val\n\ntest_text = X_test\ntest_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bert tf-hub\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport os\nimport re\nimport numpy as np\nfrom tqdm import tqdm_notebook\n#from tensorflow.keras import backend as K\nfrom keras import backend as K\nfrom keras.layers import Layer\n\n\nclass BertLayer(Layer):\n    \n    '''BertLayer which support next output_representation param:\n    \n    pooled_output: the first CLS token after adding projection layer () with shape [batch_size, 768]. \n    sequence_output: all tokens output with shape [batch_size, max_length, 768].\n    mean_pooling: mean pooling of all tokens output [batch_size, max_length, 768].\n    \n    \n    You can simple fine-tune last n layers in BERT with n_fine_tune_layers parameter. For view trainable parameters call model.trainable_weights after creating model.\n    \n    '''\n    \n    def __init__(self, n_fine_tune_layers=10, tf_hub = None, output_representation = 'pooled_output', trainable = False, **kwargs):\n        \n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.is_trainble = trainable\n        self.output_size = 768\n        self.tf_hub = tf_hub\n        self.output_representation = output_representation\n        self.supports_masking = True\n        \n        super(BertLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.bert = hub.Module(\n            self.tf_hub,\n            trainable=self.is_trainble,\n            name=\"{}_module\".format(self.name)\n        )\n        \n        \n        variables = list(self.bert.variable_map.values())\n        if self.is_trainble:\n            # 1 first remove unused layers\n            trainable_vars = [var for var in variables if not \"/cls/\" in var.name]\n            \n            \n            if self.output_representation == \"sequence_output\" or self.output_representation == \"mean_pooling\":\n                # 1 first remove unused pooled layers\n                trainable_vars = [var for var in trainable_vars if not \"/pooler/\" in var.name]\n                \n            # Select how many layers to fine tune\n            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n            \n            # Add to trainable weights\n            for var in trainable_vars:\n                self._trainable_weights.append(var)\n\n            # Add non-trainable weights\n            for var in self.bert.variables:\n                if var not in self._trainable_weights:\n                    self._non_trainable_weights.append(var)\n                \n        else:\n             for var in variables:\n                self._non_trainable_weights.append(var)\n                \n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n        \n        if self.output_representation == \"pooled_output\":\n            pooled = result[\"pooled_output\"]\n            \n        elif self.output_representation == \"mean_pooling\":\n            result_tmp = result[\"sequence_output\"]\n        \n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result_tmp, input_mask)\n            \n        elif self.output_representation == \"sequence_output\":\n            \n            pooled = result[\"sequence_output\"]\n       \n        return pooled\n    \n    def compute_mask(self, inputs, mask=None):\n        \n        if self.output_representation == 'sequence_output':\n            inputs = [K.cast(x, dtype=\"bool\") for x in inputs]\n            mask = inputs[1]\n            \n            return mask\n        else:\n            return None\n        \n        \n    def compute_output_shape(self, input_shape):\n        if self.output_representation == \"sequence_output\":\n            return (input_shape[0][0], input_shape[0][1], self.output_size)\n        else:\n            return (input_shape[0][0], self.output_size)","execution_count":12,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(max_seq_length, tf_hub, n_classes, n_fine_tune): \n    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n    \n    bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', trainable = True)(bert_inputs)\n    drop = keras.layers.Dropout(0.3)(bert_output)\n    dense = keras.layers.Dense(256, activation='sigmoid')(drop)\n    drop = keras.layers.Dropout(0.3)(dense)\n    dense = keras.layers.Dense(64, activation='sigmoid')(drop)\n    pred = keras.layers.Dense(n_classes, activation='softmax')(dense)\n    \n    model = keras.models.Model(inputs=bert_inputs, outputs=pred)\n    Adam = keras.optimizers.Adam(lr = 0.0005)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = len(label_encoder.classes_)\nn_fine_tune_layers = 48\nmodel = build_model(max_seq_length, bert_path, n_classes, n_fine_tune_layers)\n\n# Instantiate variables\ninitialize_vars(sess)","execution_count":15,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"},{"output_type":"stream","text":"W0418 04:25:36.984525 140140242679168 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stderr"},{"output_type":"stream","text":"INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","name":"stdout"},{"output_type":"stream","text":"I0418 04:25:39.493424 140140242679168 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","name":"stdout"},{"output_type":"stream","text":"W0418 04:25:39.652037 140140242679168 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","name":"stderr"},{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          (None, 128)          0                                            \n__________________________________________________________________________________________________\ninput_masks (InputLayer)        (None, 128)          0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        (None, 128)          0                                            \n__________________________________________________________________________________________________\nbert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n                                                                 input_masks[0][0]                \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 768)          0           bert_layer_1[0][0]               \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          196864      dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 64)           16448       dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 10)           650         dense_2[0][0]                    \n==================================================================================================\nTotal params: 110,318,852\nTrainable params: 21,477,578\nNon-trainable params: 88,841,274\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.trainable_weights","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[<tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_10/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/query/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/query/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/key/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/key/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/value/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/self/value/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/output/dense/kernel:0' shape=(768, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/attention/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/intermediate/dense/kernel:0' shape=(768, 3072) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/intermediate/dense/bias:0' shape=(3072,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/output/dense/kernel:0' shape=(3072, 768) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/output/dense/bias:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/output/LayerNorm/beta:0' shape=(768,) dtype=float32>,\n <tf.Variable 'bert_layer_1_module/bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>,\n <tf.Variable 'dense_1/kernel:0' shape=(768, 256) dtype=float32_ref>,\n <tf.Variable 'dense_1/bias:0' shape=(256,) dtype=float32_ref>,\n <tf.Variable 'dense_2/kernel:0' shape=(256, 64) dtype=float32_ref>,\n <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32_ref>,\n <tf.Variable 'dense_3/kernel:0' shape=(64, 10) dtype=float32_ref>,\n <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32_ref>]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization "},{"metadata":{"trusted":true},"cell_type":"code","source":"class PaddingInputExample(object):\n    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n  When running eval/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won't be generated.\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  \"\"\"\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\ndef create_tokenizer_from_hub_module(tf_hub):\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n    bert_module =  hub.Module(tf_hub)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    vocab_file, do_lower_case = sess.run(\n        [\n            tokenization_info[\"vocab_file\"],\n            tokenization_info[\"do_lower_case\"],\n        ]\n    )\n    \n    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ndef convert_single_example(tokenizer, example, max_seq_length=256):\n    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n    if isinstance(example, PaddingInputExample):\n        input_ids = [0] * max_seq_length\n        input_mask = [0] * max_seq_length\n        segment_ids = [0] * max_seq_length\n        label = 0\n        return input_ids, input_mask, segment_ids, label\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n    \n    #print(tokens)\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return input_ids, input_mask, segment_ids, example.label\n\ndef convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n\ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate tokenizer\ntokenizer = create_tokenizer_from_hub_module(bert_path)\n\n# Convert data to InputExample format\ntrain_examples = convert_text_to_examples(train_text, train_label)\nval_examples = convert_text_to_examples(val_text, val_label)\n\n# Convert to features\n(train_input_ids, train_input_masks, train_segment_ids, train_labels \n) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n(val_input_ids, val_input_masks, val_segment_ids, val_labels\n) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=max_seq_length)\n\n","execution_count":18,"outputs":[{"output_type":"stream","text":"INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","name":"stdout"},{"output_type":"stream","text":"I0418 04:25:45.851674 140140242679168 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Converting examples to features', max=31238, style=ProgressSt…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fb985edf1047cb8c24531c3641ff81"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Converting examples to features', max=3905, style=ProgressSty…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d84923fd7584c28aa9b7b87fe3d705a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nBATCH_SIZE = 256\nMONITOR = 'val_sparse_categorical_accuracy'\nprint('BATCH_SIZE is {}'.format(BATCH_SIZE))\ne_stopping = EarlyStopping(monitor=MONITOR, patience=3, verbose=1, mode='max', restore_best_weights=True)\ncallbacks =  [e_stopping]\n\nhistory = model.fit(\n   [train_input_ids, train_input_masks, train_segment_ids], \n    train_labels,\n    validation_data = ([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n    epochs = 10,\n    verbose = 1,\n    batch_size = BATCH_SIZE,\n    callbacks= callbacks\n)","execution_count":19,"outputs":[{"output_type":"stream","text":"BATCH_SIZE is 256\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stdout"},{"output_type":"stream","text":"W0418 04:28:48.591721 140140242679168 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stderr"},{"output_type":"stream","text":"Train on 31238 samples, validate on 3905 samples\nEpoch 1/10\n31238/31238 [==============================] - 220s 7ms/step - loss: 1.1961 - sparse_categorical_accuracy: 0.6434 - val_loss: 0.8705 - val_sparse_categorical_accuracy: 0.7357\nEpoch 2/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.7301 - sparse_categorical_accuracy: 0.7805 - val_loss: 0.7344 - val_sparse_categorical_accuracy: 0.7611\nEpoch 3/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.5603 - sparse_categorical_accuracy: 0.8294 - val_loss: 1.1735 - val_sparse_categorical_accuracy: 0.6402\nEpoch 4/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.5565 - sparse_categorical_accuracy: 0.8264 - val_loss: 0.6818 - val_sparse_categorical_accuracy: 0.7759\nEpoch 5/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.3825 - sparse_categorical_accuracy: 0.8844 - val_loss: 0.7414 - val_sparse_categorical_accuracy: 0.7662\nEpoch 6/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.2700 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.7986 - val_sparse_categorical_accuracy: 0.7654\nEpoch 7/10\n31238/31238 [==============================] - 212s 7ms/step - loss: 0.2060 - sparse_categorical_accuracy: 0.9415 - val_loss: 0.8969 - val_sparse_categorical_accuracy: 0.7488\nRestoring model weights from the end of the best epoch\nEpoch 00007: early stopping\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_examples = convert_text_to_examples(test_text, np.zeros(len(test_text)))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_input_ids, test_input_masks, test_segment_ids, test_labels\n) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Converting examples to features', max=3905, style=ProgressSty…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122d802dcec749a88d83b1b396fdc1e0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict([test_input_ids, test_input_masks, test_segment_ids], verbose = 1)","execution_count":22,"outputs":[{"output_type":"stream","text":"3905/3905 [==============================] - 20s 5ms/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = label_encoder.classes_[np.argmax(prediction, axis =1)]","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, preds))","execution_count":25,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.85      0.88      0.86       583\n           1       0.80      0.67      0.73       449\n           2       1.00      1.00      1.00        34\n           3       0.79      0.81      0.80       466\n           4       0.64      0.74      0.69       349\n           5       0.76      0.79      0.77       482\n           6       0.71      0.72      0.72       429\n           7       0.91      0.64      0.75       321\n           8       0.83      0.79      0.81       405\n           9       0.70      0.84      0.76       387\n\n   micro avg       0.78      0.78      0.78      3905\n   macro avg       0.80      0.79      0.79      3905\nweighted avg       0.78      0.78      0.77      3905\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, preds)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"0.7751600512163892"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(preds, columns=['label']).to_csv('bert_keras_submission.csv',\n                                                  index_label='id')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":4}