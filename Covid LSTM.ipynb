{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Covid LSTM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Mjr_GU0S9TMJQ1qjLn_okShmjrAslLvE","authorship_tag":"ABX9TyOekN2onzW3ArTdG7H4VBRk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ziBEd-1fn324","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import matplotlib.pyplot as plt\n","from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D\n","from keras.layers import Dropout, Dense, Activation, Flatten,Conv1D, SpatialDropout1D\n","from keras.models import Sequential\n","from keras.optimizers import RMSprop \n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JO3VhGrZZkk9","colab_type":"code","colab":{}},"source":["max_len = 126\n","max_words = 10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECfz7lBWq6Zg","colab_type":"code","colab":{}},"source":["df = pd.read_csv('/content/drive/My Drive/MyCovid/abstract_clusters.csv',encoding='utf-8',error_bad_lines=False,engine='python')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5qjoU1M3STx","colab_type":"code","colab":{}},"source":["df.dropna(inplace=True)\n","df.reset_index(drop=True, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jCJxa6Arnjfi","colab_type":"code","colab":{}},"source":["from sklearn.utils import shuffle\n","df = shuffle(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EI4nTx3KGWfM","colab_type":"code","colab":{}},"source":["!unzip \"/content/drive/My Drive/Paragram/glove.840B.300d.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAmd7USremmG","colab_type":"code","colab":{}},"source":["X =df['Abstract'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7P732keRenPG","colab_type":"code","colab":{}},"source":["y =df['Label'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"boL6JqmTcI-O","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X)\n","    # convert texts to sequences\n","sequences = tokenizer.texts_to_sequences(X)\n","    # generate work index\n","word_index = tokenizer.word_index\n","    # print top words count\n","print('{} of unique tokens found'.format(len(word_index)))\n","    # pad sequences using max_len param\n","X = pad_sequences(sequences, maxlen=max_len)\n","    # convert list of labels into numpy array\n","# labels = np.asarray(labels)\n","    # print shape of text and label tensors\n","# print('data tensor shape: {}\\nlabel tensor shape:{}'.format(data.shape, labels.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ri6L2N-YrJ8J","colab_type":"code","colab":{}},"source":["embeddings_index = {}\n","        # open embeddings file\n","try:\n","    f = open('glove.840B.300d.txt')\n","            # iterate over lines and split on individual words\n","            # split coefficient of word values\n","            # map words and coefficients to embeddings dictionary\n","    for line in f:\n","        values = line.split(' ') # returns list of [word, coeff]\n","        word = values[0] # gets first list element\n","        coeff = np.asarray(values[1:], dtype='float32')  # slice coefficiennt value array from remainder of list\n","                # assign mapping to dictionary\n","        embeddings_index[word] = coeff\n","    f.close()\n","except IOError:\n","    print('cannot read file. check file paths')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhLgeN06ol8f","colab_type":"code","colab":{}},"source":["       # prepare glove word-embedding matrix\n","        # create empty embedding tensor\n","embedding_matrix = np.zeros((max_words,300))\n","        # map the top words of the data into the glove embedding matrix\n","        # words not found from the data in glove will be zeroed\n","for word, i in word_index.items():\n","        if i >= max_words: continue\n","        embedding_vector = embeddings_index.get(word)\n","        #ALLmight\n","        if embedding_vector is not None: \n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            embedding_vector = embeddings_index.get(word.capitalize())\n","            if embedding_vector is not None: \n","                embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTyCLJwleR5Q","colab_type":"code","colab":{}},"source":["X_train, X_val_test, y_train, y_val_test = train_test_split(\n","        X,y, test_size=0.2, random_state=0, stratify = y\n","        )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHQF4QHLeaYR","colab_type":"code","colab":{}},"source":["X_val, X_test, y_val, y_test = train_test_split(\n","        X_val_test,y_val_test, test_size=0.5, random_state=0, stratify = y_val_test\n","        )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMJw9mPJr58d","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Embedding(max_words, 300, weights=[embedding_matrix], trainable=True))\n","model.add(Conv1D(64, 3, activation='relu'))\n","model.add(MaxPooling1D(4))\n","model.add(LSTM(64, dropout=0.1)) \n","model.add(Dense(10, activation='softmax'))\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zfRtDpkr9rK","colab_type":"code","colab":{}},"source":["Adam = keras.optimizers.Adam(lr = 0.0005)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyaOZr0dsArV","colab_type":"code","colab":{}},"source":["history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxgKB9qYIMYY","colab_type":"code","colab":{}},"source":["prediction = model.predict(X_test, verbose = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RPCGyIOINBr","colab_type":"code","colab":{}},"source":["preds = np.argmax(prediction, axis =1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIkomcLnIgCJ","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test, preds))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTvZV58PIqxY","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, preds)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OV8Y8nSsFi8","colab_type":"code","colab":{}},"source":["# define plotting metrics\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","# plot model training and validation accuracy and loss\n","plot_training_and_validation(acc, val_acc, loss, val_loss)"],"execution_count":0,"outputs":[]}]}